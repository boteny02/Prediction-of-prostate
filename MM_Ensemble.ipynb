{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boteny02/Prediction-of-prostate/blob/main/MM_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymp8rPNELXxB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWyzCzmURjQS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea451e3e"
      },
      "source": [
        "# Task\n",
        "Implement a medical image classification pipeline using the \"MRI_Metadata.xlsx\" file and corresponding MRI images. The pipeline will involve feature extraction with a pre-trained ResNet model, feature selection using the ReliefF algorithm, training a suitable classifier, and evaluating its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe0ea86"
      },
      "source": [
        "## Load and Preprocess Metadata\n",
        "\n",
        "### Subtask:\n",
        "Load the /content/drive/MyDrive/PhD_dataset/file_details_with_labels.xlsx file using pandas to inspect its structure and content. Perform any necessary initial data cleaning, handle missing values, and prepare the metadata for linking with image files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewSVkTgZjkJK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89356212"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the MRI_Metadata.xlsx file into a pandas DataFrame\n",
        "df_metadata = pd.read_excel('/content/drive/MyDrive/PhD_dataset/file_details_with_labels.xlsx')\n",
        "\n",
        "# Display the first five rows of the DataFrame\n",
        "print('First 5 rows of the DataFrame:')\n",
        "print(df_metadata.head())\n",
        "print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOWeBeI9loUz"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/PhD_dataset/prostate_cancer_prediction.csv')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzNV-8bWhpyN"
      },
      "outputs": [],
      "source": [
        "df_metadata.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u1m9jh1_Od6"
      },
      "outputs": [],
      "source": [
        "df_metadata.value_counts('Label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "957b37b3"
      },
      "source": [
        "# Task\n",
        "Implement a medical image classification pipeline using the metadata from `/content/drive/MyDrive/PhD_dataset/file_details_with_labels.xlsx` and the corresponding MRI images. This pipeline should include loading and preprocessing the metadata, preparing image data paths, downloading and extracting image data, defining and loading a pre-trained ResNet model for feature extraction, extracting features from the MRI images, applying ReliefF for feature selection, training a suitable classifier, and evaluating the model's performance with appropriate metrics for medical image classification. Finally, summarize the entire pipeline and results in a note cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b52463a2"
      },
      "source": [
        "## Load and Preprocess Metadata\n",
        "\n",
        "### Subtask:\n",
        "Perform initial data cleaning and prepare metadata for linking with image files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd881d9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires checking for and handling duplicate rows, analyzing the distribution of the 'Label' column, and examining unique values in the 'ExamDescription' column. These steps will be performed sequentially in a single code block.\n",
        "\n",
        "**Reasoning**:\n",
        "The 'ExamDescription' column contains several similar but not identical values (e.g., 'PROSTATE', 'PROSTATE ', 'PROSTATE BIOPSY', 'PROSTATE BX'). Standardizing these values will improve data consistency and facilitate easier analysis and filtering. This step is part of the initial data cleaning process to prepare the metadata.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf1e5bbb"
      },
      "outputs": [],
      "source": [
        "print('Checking for duplicate rows...')\n",
        "duplicate_rows = df_metadata.duplicated().sum()\n",
        "\n",
        "if duplicate_rows > 0:\n",
        "    print(f'Found {duplicate_rows} duplicate rows. Dropping duplicates...')\n",
        "    df_metadata.drop_duplicates(inplace=True)\n",
        "    print(f'New DataFrame shape after dropping duplicates: {df_metadata.shape}')\n",
        "else:\n",
        "    print('No duplicate rows found.')\n",
        "\n",
        "# Clean up whitespace and convert to uppercase for consistency\n",
        "df_metadata['ExamDescription'] = df_metadata['ExamDescription'].str.strip().str.upper()\n",
        "\n",
        "# Standardize similar descriptions\n",
        "df_metadata['ExamDescription'] = df_metadata['ExamDescription'].replace({\n",
        "    'PROSTATE BX': 'PROSTATE BIOPSY',\n",
        "    'PROSTATE -NO COIL': 'PROSTATE',\n",
        "    'PROSTATE  W/ SPEC': 'PROSTATE'\n",
        "})\n",
        "\n",
        "print('\\nAnalyzing the distribution of the Label column:')\n",
        "print(df_metadata['Label'].value_counts())\n",
        "\n",
        "print('\\nExamining unique values in the ExamDescription column:')\n",
        "print(df_metadata['ExamDescription'].unique())\n",
        "\n",
        "print('\\nValue counts for ExamDescription after standardization:')\n",
        "print(df_metadata['ExamDescription'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67e2ae68"
      },
      "source": [
        "# Task\n",
        "## Prepare Image Data Paths\n",
        "\n",
        "### Subtask:\n",
        "Verify the existence and accessibility of the image paths specified in the 'File_Path' column of the `df_metadata` DataFrame. This involves checking if each file path points to an actual file on the system.\n",
        "\n",
        "### Reasoning:\n",
        "Before proceeding with image loading and feature extraction, it's crucial to confirm that all referenced image files are accessible. This step identifies any broken paths or missing files early in the pipeline, preventing errors in subsequent stages and ensuring that only valid image data is processed. We will iterate through the `File_Path` column, check each path, and report on the overall accessibility. If any file paths are invalid, they should be identified and handled, potentially by removing the corresponding entries from the metadata or flagging them for further investigation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02b25ed"
      },
      "source": [
        "## Prepare Image Data Paths\n",
        "\n",
        "### Subtask:\n",
        "Verify the existence and accessibility of the image paths specified in the 'File_Path' column of the `df_metadata` DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f31d348e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires verifying the existence and accessibility of image paths. This involves importing the `os` module, defining a function to check file existence, applying this function to the 'File_Path' column to create a new 'File_Exists' column, summarizing the results, and then filtering the DataFrame to retain only valid paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5d3f8b5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define a function to check if a file exists\n",
        "def check_file_exists(file_path):\n",
        "    return os.path.exists(file_path)\n",
        "\n",
        "print(\"Verifying file paths...\")\n",
        "# Apply the function to the 'File_Path' column to create 'File_Exists'\n",
        "df_metadata['File_Exists'] = df_metadata['File_Path'].apply(check_file_exists)\n",
        "\n",
        "# Count and print the number of True and False values in 'File_Exists'\n",
        "print('\\nFile existence check summary:')\n",
        "print(df_metadata['File_Exists'].value_counts())\n",
        "\n",
        "# Create a new DataFrame with only valid paths\n",
        "df_metadata_valid_paths = df_metadata[df_metadata['File_Exists'] == True].copy()\n",
        "\n",
        "print(f'\\nOriginal DataFrame shape: {df_metadata.shape}')\n",
        "print(f'DataFrame with valid paths shape: {df_metadata_valid_paths.shape}')\n",
        "\n",
        "print('\\nFirst 5 rows of df_metadata_valid_paths:')\n",
        "print(df_metadata_valid_paths.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MtHThlwoa4d"
      },
      "source": [
        "## Apply StandardScaler\n",
        "\n",
        "### Subtask:\n",
        "Initialize the `StandardScaler` and apply it to the identified numerical columns in both `x_train` and `x_test`. The scaler should be fitted only on `x_train` to prevent data leakage.\n",
        "\n",
        "\n",
        "**Reasoning**:\n",
        "To scale the numerical features, I will import `StandardScaler`, instantiate it, fit it on the training numerical data (`x_train_numerical`), and then transform both the training and testing numerical data (`x_train_numerical` and `x_test_numerical`) to `x_train_scaled` and `x_test_scaled` respectively, ensuring no data leakage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRBv7imsoSod"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f6ddbe"
      },
      "source": [
        "## Define and Load CNN Model for Feature Extraction (ResNet)\n",
        "\n",
        "### Subtask:\n",
        "Load a pre-trained ResNet model, specifically ResNet50, and modify it by removing its final classification layers to use it as a feature extractor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54986888"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the necessary Keras modules, load the pre-trained ResNet50 model without its top classification layer, and then define a new model that applies global average pooling to the ResNet50's output to serve as a feature extractor. This will be done in a single code block as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "38f151e2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "print(\"Loading pre-trained ResNet50 model...\")\n",
        "# 1. Load the pre-trained ResNet50 model\n",
        "#    include_top=False removes the final classification layers\n",
        "#    weights='imagenet' loads the weights pre-trained on ImageNet\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# 2. Create a new model for feature extraction\n",
        "#    Take the output of the base model and apply GlobalAveragePooling2D\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Define the feature extraction model\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "print(\"ResNet50 feature extractor loaded and configured.\")\n",
        "print(\"Summary of the feature extractor model:\")\n",
        "feature_extractor.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59b35178"
      },
      "source": [
        "## Extract Features from MRI Images\n",
        "\n",
        "### Subtask:\n",
        "Process the MRI images to extract features using the pre-trained and modified ResNet50 model (`feature_extractor`). This involves loading each image, preprocessing it to match the input requirements of ResNet50, and then using the model to obtain feature vectors.\n",
        "\n",
        "#### Instructions\n",
        "1.  Define a function to load and preprocess a single image file. This function should:\n",
        "    *   Load the image using `tf.keras.preprocessing.image.load_img`.\n",
        "    *   Resize the image to (224, 224) as required by ResNet50.\n",
        "    *   Convert the image to a NumPy array.\n",
        "    *   Expand dimensions to create a batch dimension (e.g., from (224, 224, 3) to (1, 224, 224, 3)).\n",
        "    *   Preprocess the image using `tf.keras.applications.resnet50.preprocess_input`.\n",
        "2.  Iterate through the `File_Path` column of `df_metadata_valid_paths`.\n",
        "3.  For each image path, load and preprocess the image using the function defined in step 1.\n",
        "4.  Use the `feature_extractor.predict()` method to extract features for each preprocessed image.\n",
        "5.  Store the extracted features in a list or a NumPy array.\n",
        "6.  Ensure that the extracted features are reshaped if necessary to be 2D (number of samples, number of features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35655729"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous markdown block detailed the instructions for extracting features from the MRI images using the pre-trained ResNet50 model. This code block will implement those instructions by defining an image preprocessing function, iterating through the image paths in the DataFrame, extracting features for each image, and storing them in a NumPy array.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c4a5dcd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define a function to load and preprocess a single image\n",
        "def load_and_preprocess_image(image_path):\n",
        "    try:\n",
        "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "        img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Extracting features from MRI images...\")\n",
        "\n",
        "# List to store extracted features\n",
        "extracted_features = []\n",
        "\n",
        "# Iterate through the DataFrame and extract features for each image\n",
        "for index, row in tqdm(df_metadata_valid_paths.iterrows(), total=len(df_metadata_valid_paths)):\n",
        "    image_path = row['File_Path']\n",
        "    preprocessed_img = load_and_preprocess_image(image_path)\n",
        "\n",
        "    if preprocessed_img is not None:\n",
        "        features = feature_extractor.predict(preprocessed_img, verbose=0) # Use verbose=0 to suppress output for each prediction\n",
        "        extracted_features.append(features.flatten()) # Flatten the features to a 1D array\n",
        "\n",
        "# Convert the list of features to a NumPy array\n",
        "features_array = np.array(extracted_features)\n",
        "\n",
        "print(f\"Finished feature extraction. Total features extracted: {len(extracted_features)}\")\n",
        "print(f\"Shape of the extracted features array: {features_array.shape}\")\n",
        "\n",
        "# Optionally, add features to the DataFrame\n",
        "df_metadata_valid_paths['Features'] = list(extracted_features)\n",
        "print(\"Features successfully extracted and stored in 'features_array' and 'df_metadata_valid_paths'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b977909"
      },
      "source": [
        "# Task\n",
        "## Apply ReliefF Feature Selection\n",
        "\n",
        "### Subtask:\n",
        "Utilize the ReliefF algorithm to select the most relevant features from the `features_array`. This involves importing a ReliefF implementation, fitting it to the feature data and labels, and obtaining feature importances to select a subset of discriminative features for downstream classification.\n",
        "\n",
        "### Reasoning:\n",
        "ReliefF is a feature selection algorithm that assigns weights to features based on their ability to distinguish between neighboring instances from different classes. This helps in identifying features that are more discriminative for the classification task, potentially improving model performance and reducing dimensionality. The `skrebate` library provides a suitable implementation for ReliefF. We will fit ReliefF to the extracted features and the corresponding labels, then use the calculated feature importances to select a subset of the most relevant features. This step is crucial for preparing the data for the classifier training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95c71e98"
      },
      "source": [
        "**Reasoning**:\n",
        "To proceed with ReliefF feature selection, I first need to install the `scikit-rebate` library, which contains the algorithm. After installation, I will extract the target labels and then apply the ReliefF algorithm as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a543810"
      },
      "outputs": [],
      "source": [
        "print(\"Installing skrebate...\")\n",
        "!pip install skrebate\n",
        "\n",
        "import numpy as np\n",
        "from skrebate import ReliefF\n",
        "\n",
        "# Prepare data for ReliefF\n",
        "X = features_array  # Our extracted features\n",
        "y = df_metadata_valid_paths['Label'].values  # Target labels\n",
        "\n",
        "print(f\"Shape of features (X): {X.shape}\")\n",
        "print(f\"Shape of labels (y): {y.shape}\")\n",
        "\n",
        "# Instantiate ReliefF model\n",
        "# n_neighbors: The number of nearest neighbors to consider\n",
        "print(\"Instantiating and fitting ReliefF model...\")\n",
        "# Removed n_features_to_keep and random_state from constructor as they are not valid arguments\n",
        "relief = ReliefF(n_neighbors=10)\n",
        "\n",
        "# Fit ReliefF to the data\n",
        "relief.fit(X, y)\n",
        "\n",
        "# Get all feature indices sorted by importance, then select the top N\n",
        "desired_n_features = 200\n",
        "sorted_feature_indices = relief.top_features_\n",
        "selected_feature_indices = sorted_feature_indices[:desired_n_features]\n",
        "\n",
        "# Create a new feature set with only the selected features\n",
        "selected_features = X[:, selected_feature_indices]\n",
        "\n",
        "print(f\"Original number of features: {X.shape[1]}\")\n",
        "print(f\"Number of features selected by ReliefF: {selected_features.shape[1]}\")\n",
        "print(f\"Shape of selected features array: {selected_features.shape}\")\n",
        "\n",
        "print(\"ReliefF feature selection completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3600628d"
      },
      "source": [
        "## Train Classifier\n",
        "\n",
        "### Subtask:\n",
        "Using the features selected by the ReliefF algorithm, split the dataset into training and testing sets. Then, train a suitable machine learning classifier for medical image classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07129534"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare for model training and evaluation, I need to split the data into training and testing sets and then train a RandomForestClassifier as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOFKCIUs1p7-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G-VTDFQzhe6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"Splitting data into training and testing sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# Assume the dataset has a 'MRI' column and a 'cancer' column (1 for cancer, 0 for no cancer)\n",
        "# Adjust column names based on your actual dataset\n",
        "\n",
        "# Initialize individual models\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "svm_model = SVC(probability=True, random_state=42) # probability=True for VotingClassifier with soft voting\n",
        "knn_model = KNeighborsClassifier()\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Build the ANN model\n",
        "ann_model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train individual models\n",
        "print(\"Training individual models...\")\n",
        "dt_model.fit(X_train_scaled, y_train)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "nb_model.fit(X_train_scaled, y_train)\n",
        "ann_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0) # Train ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6cd552c5"
      },
      "outputs": [],
      "source": [
        "# Ensure skrebate is installed for ReliefF feature selection\n",
        "!pip install skrebate\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "import numpy as np\n",
        "from skrebate import ReliefF\n",
        "\n",
        "# Original imports for this cell\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Code from Cell 38f151e2: Define and Load CNN Model for Feature Extraction (ResNet) ---\n",
        "print(\"Loading pre-trained ResNet50 model for feature extraction...\")\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
        "print(\"ResNet50 feature extractor configured.\")\n",
        "\n",
        "# --- Code from Cell f5d3f8b5: Prepare Image Data Paths ---\n",
        "# df_metadata is assumed to be available from previously executed cells (e.g., 89356212, bf1e5bbb)\n",
        "print(\"Verifying file paths...\")\n",
        "def check_file_exists(file_path):\n",
        "    return os.path.exists(file_path)\n",
        "df_metadata['File_Exists'] = df_metadata['File_Path'].apply(check_file_exists)\n",
        "df_metadata_valid_paths = df_metadata[df_metadata['File_Exists'] == True].copy()\n",
        "print(f\"DataFrame with valid paths shape: {df_metadata_valid_paths.shape}\")\n",
        "\n",
        "# --- Code from Cell 9c4a5dcd: Extract Features from MRI Images ---\n",
        "print(\"Extracting features from MRI images...\")\n",
        "def load_and_preprocess_image(image_path):\n",
        "    try:\n",
        "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        # print(f\"Error processing image {image_path}: {e}\") # Suppress detailed error for combined cell\n",
        "        return None\n",
        "\n",
        "extracted_features = []\n",
        "# Using iterrows without tqdm for simplicity in combined cell\n",
        "for index, row in df_metadata_valid_paths.iterrows():\n",
        "    image_path = row['File_Path']\n",
        "    preprocessed_img = load_and_preprocess_image(image_path)\n",
        "    if preprocessed_img is not None:\n",
        "        features = feature_extractor.predict(preprocessed_img, verbose=0)\n",
        "        extracted_features.append(features.flatten())\n",
        "features_array = np.array(extracted_features)\n",
        "print(f\"Finished feature extraction. Total features extracted: {len(extracted_features)}\")\n",
        "print(f\"Shape of the extracted features array: {features_array.shape}\")\n",
        "\n",
        "# --- Code from Cell 7a543810: Apply ReliefF Feature Selection ---\n",
        "print(\"Instantiating and fitting ReliefF model...\")\n",
        "X_relief = features_array\n",
        "y = df_metadata_valid_paths['Label'].values\n",
        "\n",
        "relief = ReliefF(n_neighbors=10)\n",
        "relief.fit(X_relief, y)\n",
        "\n",
        "desired_n_features = 200\n",
        "sorted_feature_indices = relief.top_features_\n",
        "selected_feature_indices = sorted_feature_indices[:desired_n_features]\n",
        "selected_features = X_relief[:, selected_feature_indices]\n",
        "\n",
        "print(f\"Number of features selected by ReliefF: {selected_features.shape[1]}\")\n",
        "print(f\"Shape of selected features array: {selected_features.shape}\")\n",
        "print(\"ReliefF feature selection completed.\")\n",
        "\n",
        "# --- Original Content of this cell (6cd552c5): Train Classifier ---\n",
        "\n",
        "print(\"Splitting data into training and testing sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"Scaling features using StandardScaler...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "\n",
        "print(\"Instantiating and training RandomForestClassifier...\")\n",
        "classifier = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"RandomForestClassifier trained successfully on scaled data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY3gH1V2qke0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi-80KV7qjMV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27159376"
      },
      "source": [
        "# Task\n",
        "The next step is to preprocess the tabular data (`df` from 'prostate_cancer_prediction.csv'). This involves handling any missing values, encoding categorical features, and scaling numerical features to prepare them for integration with the image features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1b36289"
      },
      "source": [
        "## Preprocess Tabular Data\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the 'prostate_cancer_prediction.csv' tabular data, handling missing values, encoding categorical features, and scaling numerical features to prepare them for integration with the image features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1316a2"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading and preprocessing the 'prostate_cancer_prediction.csv' tabular data. This involves handling missing values (imputing numerical with mean, categorical with mode), encoding categorical features (dropping Patient_ID, LabelEncoding binary, and mapping ordinal), and scaling numerical features using StandardScaler. These steps will be performed sequentially in a single code block to complete the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77c47bd7"
      },
      "source": [
        "print(\"Starting preprocessing of tabular data...\")\n",
        "\n",
        "# --- 1. Handle Missing Values ---\n",
        "print(\"Handling missing values...\")\n",
        "# Numerical columns to impute with mean\n",
        "numerical_cols_for_imputation = ['Age', 'PSA_Level', 'Screening_Age', 'Prostate_Volume']\n",
        "for col in numerical_cols_for_imputation:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "        print(f\"Filled missing values in '{col}' with mean.\")\n",
        "\n",
        "# Categorical columns to impute with mode\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "# Exclude Patient_ID, as it will be dropped\n",
        "categorical_cols_for_imputation = [col for col in categorical_cols if col not in ['Patient_ID']]\n",
        "\n",
        "for col in categorical_cols_for_imputation:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "        print(f\"Filled missing values in '{col}' with mode.\")\n",
        "\n",
        "print(\"Missing values handled.\")\n",
        "\n",
        "# --- 2. Encode Categorical Features ---\n",
        "print(\"Encoding categorical features...\")\n",
        "# Drop Patient_ID\n",
        "df.drop('Patient_ID', axis=1, inplace=True)\n",
        "print(\"Dropped 'Patient_ID' column.\")\n",
        "\n",
        "# Binary categorical columns for Label Encoding (Yes/No, Normal/Abnormal, Benign/Malignant)\n",
        "binary_cols = [\n",
        "    'Family_History', 'Race_African_Ancestry', 'DRE_Result', 'Biopsy_Result',\n",
        "    'Difficulty_Urinating', 'Weak_Urine_Flow', 'Blood_in_Urine', 'Prostate_Pain',\n",
        "    'Urinary_Frequency', 'Nocturia', 'Erectile_Dysfunction', 'Weight_Loss',\n",
        "    'Bone_Pain', 'Fatigue', 'Back_Pain', 'Smoking', 'Hypertension', 'Diabetes',\n",
        "    'Follow_Up_Required', 'Genetic_Risk_Factors', 'Previous_Cancer_History', 'Early_Detection'\n",
        "]\n",
        "\n",
        "for col in binary_cols:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        # Handle cases where 'No' might be default, ensuring consistent 0/1 encoding\n",
        "        unique_vals = df[col].unique()\n",
        "        if 'Yes' in unique_vals and 'No' in unique_vals:\n",
        "            # Ensure 'No' maps to 0 and 'Yes' maps to 1\n",
        "            mapping = {'No': 0, 'Yes': 1}\n",
        "            df[col] = df[col].map(mapping)\n",
        "        elif 'Normal' in unique_vals and 'Abnormal' in unique_vals:\n",
        "            mapping = {'Normal': 0, 'Abnormal': 1}\n",
        "            df[col] = df[col].map(mapping)\n",
        "        elif 'Benign' in unique_vals and 'Malignant' in unique_vals:\n",
        "            mapping = {'Benign': 0, 'Malignant': 1}\n",
        "            df[col] = df[col].map(mapping)\n",
        "        else:\n",
        "            # Fallback for other binary categories if any\n",
        "            df[col] = le.fit_transform(df[col])\n",
        "        print(f\"Encoded binary column: '{col}'.\")\n",
        "\n",
        "# Ordinal categorical columns mapping\n",
        "print(\"Mapping ordinal categorical columns...\")\n",
        "# Alcohol_Consumption: 'Low': 0, 'Moderate': 1, 'High': 2\n",
        "alcohol_mapping = {'Low': 0, 'Moderate': 1, 'High': 2}\n",
        "df['Alcohol_Consumption'] = df['Alcohol_Consumption'].map(alcohol_mapping)\n",
        "print(\"Mapped 'Alcohol_Consumption'.\")\n",
        "\n",
        "# Cholesterol_Level: 'Low': 0, 'Normal': 1, 'High': 2\n",
        "cholesterol_mapping = {'Low': 0, 'Normal': 1, 'High': 2}\n",
        "df['Cholesterol_Level'] = df['Cholesterol_Level'].map(cholesterol_mapping)\n",
        "print(\"Mapped 'Cholesterol_Level'.\")\n",
        "\n",
        "print(\"Categorical features encoded.\")\n",
        "\n",
        "# --- 3. Scale Numerical Features ---\n",
        "print(\"Scaling numerical features...\")\n",
        "numerical_cols_for_scaling = ['Age', 'PSA_Level', 'Screening_Age', 'Prostate_Volume']\n",
        "\n",
        "# Ensure these columns exist and are numerical after imputation\n",
        "numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col in df.columns and np.issubdtype(df[col].dtype, np.number)]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_preprocessed_tabular = df.copy() # Create a copy to store preprocessed data\n",
        "df_preprocessed_tabular[numerical_cols_for_scaling] = scaler.fit_transform(df_preprocessed_tabular[numerical_cols_for_scaling])\n",
        "\n",
        "print(\"Numerical features scaled.\")\n",
        "\n",
        "print(\"Tabular data preprocessing complete. Displaying first 5 rows of preprocessed data:\")\n",
        "print(df_preprocessed_tabular.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db8a71cb"
      },
      "source": [
        "## Integrate Multimodal Features\n",
        "\n",
        "### Subtask:\n",
        "Combine the ReliefF selected image features (`selected_features`) with the preprocessed tabular features. This will create a single, comprehensive multimodal feature set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23e93795"
      },
      "source": [
        "### Subtask: Integrate Multimodal Features\n",
        "Combine the ReliefF selected image features (`selected_features`) with the preprocessed tabular features. This will create a single, comprehensive multimodal feature set.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "**Reasoning for Data Alignment**:\n",
        "Before combining the image features (`selected_features`) and the tabular features (`df_preprocessed_tabular`), it's crucial to address a data alignment issue. The `selected_features` are derived from `df_metadata_valid_paths`, which contains individual image entries and patient identifiers ('Patient' column). The `df_preprocessed_tabular` comes from the `prostate_cancer_prediction.csv` file and has patient-level data, but its 'Patient_ID' column was dropped during its preprocessing, and it has a different number of rows (5000) than `selected_features` (5842). Directly concatenating these arrays is not possible.\n",
        "\n",
        "To ensure correct integration, we need to:\n",
        "1.  Reload the original tabular dataset (`prostate_cancer_prediction.csv`) to recover the 'Patient_ID'.\n",
        "2.  Apply the same preprocessing steps to this reloaded data, but this time, preserve the 'Patient_ID' column.\n",
        "3.  Merge `df_metadata_valid_paths` (which has image features and 'Patient' IDs) with the preprocessed tabular data (which will now have 'Patient_ID') using the patient identifiers as keys. This step will associate each image's features with its corresponding patient's tabular data.\n",
        "4.  Extract the aligned tabular features from this merged DataFrame.\n",
        "5.  Finally, concatenate the `selected_features` (image features) with the extracted and aligned tabular features to form the complete multimodal feature set.\n",
        "6.  Verify the shape of the newly created multimodal feature set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f5b0433"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous markdown block (`23e93795`) outlined the necessity of data alignment before combining image and tabular features directly due to differences in row counts and missing identifiers. The immediate next step is to implement this alignment process by reloading the tabular data, re-applying preprocessing while preserving 'Patient_ID', and then merging with the image metadata DataFrame. This will prepare the tabular features for proper integration with the image features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a20adaf1"
      },
      "source": [
        "print(\"Reloading and preprocessing tabular data for alignment...\")\n",
        "\n",
        "# 1. Reload the original tabular dataset to recover 'Patient_ID'\n",
        "df_original_tabular = pd.read_csv('/content/drive/MyDrive/PhD_dataset/prostate_cancer_prediction.csv')\n",
        "\n",
        "# Make a copy to preserve original Patient_ID for merging\n",
        "df_preprocessed_tabular_with_id = df_original_tabular.copy()\n",
        "\n",
        "# --- Apply the same preprocessing steps as in cell 77c47bd7 ---\n",
        "\n",
        "# Handle Missing Values\n",
        "numerical_cols_for_imputation = ['Age', 'PSA_Level', 'Screening_Age', 'Prostate_Volume']\n",
        "for col in numerical_cols_for_imputation:\n",
        "    if df_preprocessed_tabular_with_id[col].isnull().any():\n",
        "        df_preprocessed_tabular_with_id[col].fillna(df_preprocessed_tabular_with_id[col].mean(), inplace=True)\n",
        "\n",
        "categorical_cols = df_preprocessed_tabular_with_id.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_for_imputation = [col for col in categorical_cols if col not in ['Patient_ID']]\n",
        "\n",
        "for col in categorical_cols_for_imputation:\n",
        "    if df_preprocessed_tabular_with_id[col].isnull().any():\n",
        "        df_preprocessed_tabular_with_id[col].fillna(df_preprocessed_tabular_with_id[col].mode()[0], inplace=True)\n",
        "\n",
        "# Encode Categorical Features (preserving Patient_ID for now)\n",
        "binary_cols = [\n",
        "    'Family_History', 'Race_African_Ancestry', 'DRE_Result', 'Biopsy_Result',\n",
        "    'Difficulty_Urinating', 'Weak_Urine_Flow', 'Blood_in_Urine', 'Prostate_Pain',\n",
        "    'Urinary_Frequency', 'Nocturia', 'Erectile_Dysfunction', 'Weight_Loss',\n",
        "    'Bone_Pain', 'Fatigue', 'Back_Pain', 'Smoking', 'Hypertension', 'Diabetes',\n",
        "    'Follow_Up_Required', 'Genetic_Risk_Factors', 'Previous_Cancer_History', 'Early_Detection'\n",
        "]\n",
        "\n",
        "for col in binary_cols:\n",
        "    if col in df_preprocessed_tabular_with_id.columns:\n",
        "        unique_vals = df_preprocessed_tabular_with_id[col].unique()\n",
        "        if 'Yes' in unique_vals and 'No' in unique_vals:\n",
        "            mapping = {'No': 0, 'Yes': 1}\n",
        "            df_preprocessed_tabular_with_id[col] = df_preprocessed_tabular_with_id[col].map(mapping)\n",
        "        elif 'Normal' in unique_vals and 'Abnormal' in unique_vals:\n",
        "            mapping = {'Normal': 0, 'Abnormal': 1}\n",
        "            df_preprocessed_tabular_with_id[col] = df_preprocessed_tabular_with_id[col].map(mapping)\n",
        "        elif 'Benign' in unique_vals and 'Malignant' in unique_vals:\n",
        "            mapping = {'Benign': 0, 'Malignant': 1}\n",
        "            df_preprocessed_tabular_with_id[col] = df_preprocessed_tabular_with_id[col].map(mapping)\n",
        "        else:\n",
        "            le = LabelEncoder()\n",
        "            df_preprocessed_tabular_with_id[col] = le.fit_transform(df_preprocessed_tabular_with_id[col])\n",
        "\n",
        "alcohol_mapping = {'Low': 0, 'Moderate': 1, 'High': 2}\n",
        "df_preprocessed_tabular_with_id['Alcohol_Consumption'] = df_preprocessed_tabular_with_id['Alcohol_Consumption'].map(alcohol_mapping)\n",
        "\n",
        "cholesterol_mapping = {'Low': 0, 'Normal': 1, 'High': 2}\n",
        "df_preprocessed_tabular_with_id['Cholesterol_Level'] = df_preprocessed_tabular_with_id['Cholesterol_Level'].map(cholesterol_mapping)\n",
        "\n",
        "# Scale Numerical Features\n",
        "numerical_cols_for_scaling = ['Age', 'PSA_Level', 'Screening_Age', 'Prostate_Volume']\n",
        "numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col in df_preprocessed_tabular_with_id.columns and np.issubdtype(df_preprocessed_tabular_with_id[col].dtype, np.number)]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_preprocessed_tabular_with_id[numerical_cols_for_scaling] = scaler.fit_transform(df_preprocessed_tabular_with_id[numerical_cols_for_scaling])\n",
        "\n",
        "print(\"Tabular data re-preprocessed with 'Patient_ID' retained.\")\n",
        "\n",
        "# 3. Merge df_metadata_valid_paths with the preprocessed tabular data\n",
        "#    Ensure Patient column in df_metadata_valid_paths is ready for merge\n",
        "\n",
        "# Create a DataFrame for image features with Patient ID\n",
        "df_image_features = pd.DataFrame(selected_features)\n",
        "df_image_features['Patient'] = df_metadata_valid_paths['Patient'].values\n",
        "df_image_features['File_Path'] = df_metadata_valid_paths['File_Path'].values\n",
        "\n",
        "print(\"Merging image features with preprocessed tabular data...\")\n",
        "# Perform the merge operation\n",
        "# We merge df_image_features (which has one row per image) with df_preprocessed_tabular_with_id (which has one row per patient)\n",
        "# The merge will duplicate tabular data for each image belonging to the same patient.\n",
        "merged_multimodal_df = pd.merge(\n",
        "    df_image_features,\n",
        "    df_preprocessed_tabular_with_id, # This now contains Patient_ID\n",
        "    left_on='Patient',\n",
        "    right_on='Patient_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Drop redundant Patient_ID column from the merged DataFrame\n",
        "merged_multimodal_df.drop('Patient_ID', axis=1, inplace=True)\n",
        "\n",
        "# Verify the merge\n",
        "print(f\"Shape of merged_multimodal_df: {merged_multimodal_df.shape}\")\n",
        "print(\"First 5 rows of merged_multimodal_df:\")\n",
        "print(merged_multimodal_df.head())\n",
        "\n",
        "# 4. Extract the aligned tabular features\n",
        "# Identify columns that are not image features or Patient/File_Path related\n",
        "tabular_feature_cols = [col for col in merged_multimodal_df.columns if col not in (list(range(selected_features.shape[1])) + ['Patient', 'File_Path'])]\n",
        "aligned_tabular_features = merged_multimodal_df[tabular_feature_cols].values\n",
        "\n",
        "print(f\"Shape of aligned_tabular_features: {aligned_tabular_features.shape}\")\n",
        "\n",
        "# 5. Concatenate selected_features (image features) with aligned_tabular_features\n",
        "# Ensure both are numpy arrays before concatenating\n",
        "multimodal_features = np.hstack((selected_features, aligned_tabular_features))\n",
        "\n",
        "# 6. Verify the shape of the newly created multimodal feature set\n",
        "print(f\"Shape of final multimodal_features: {multimodal_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28cb1c11"
      },
      "source": [
        "## Apply Multimodal ReliefF Feature Selection\n",
        "\n",
        "### Subtask:\n",
        "Apply the ReliefF algorithm to the combined multimodal feature set to select the most discriminative features from both image and tabular data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8f389e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires applying ReliefF feature selection to the combined multimodal features. This involves setting up the ReliefF model, fitting it to the multimodal features and labels, and then selecting a desired number of top features, followed by printing the resulting shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c5780b6"
      },
      "source": [
        "print(\"Applying ReliefF feature selection to multimodal features...\")\n",
        "\n",
        "# 1. Import ReliefF (already imported in previous cell, but re-including for clarity)\n",
        "from skrebate import ReliefF\n",
        "\n",
        "# 2. Define X_multimodal as the multimodal_features array and y_multimodal as the target labels (y)\n",
        "X_multimodal = multimodal_features\n",
        "y_multimodal = df_metadata_valid_paths['Label'].values # Ensure y_multimodal corresponds to the aligned data\n",
        "\n",
        "print(f\"Shape of original multimodal features (X_multimodal): {X_multimodal.shape}\")\n",
        "print(f\"Shape of multimodal labels (y_multimodal): {y_multimodal.shape}\")\n",
        "\n",
        "# 3. Instantiate the ReliefF model with a suitable n_neighbors value\n",
        "# n_neighbors=10 is a common starting point\n",
        "multimodal_relief = ReliefF(n_neighbors=10)\n",
        "\n",
        "# 4. Fit the ReliefF model to X_multimodal and y_multimodal\n",
        "print(\"Fitting ReliefF model to multimodal features...\")\n",
        "multimodal_relief.fit(X_multimodal, y_multimodal)\n",
        "\n",
        "# 5. Define the desired number of features to keep (e.g., 100 or 200)\n",
        "desired_n_multimodal_features = 100 # Can adjust this value\n",
        "\n",
        "# 6. Get the sorted feature indices from the fitted ReliefF model\n",
        "sorted_multimodal_feature_indices = multimodal_relief.top_features_\n",
        "\n",
        "# 7. Select the top N feature indices\n",
        "selected_multimodal_feature_indices = sorted_multimodal_feature_indices[:desired_n_multimodal_features]\n",
        "\n",
        "# 8. Create a new feature set with only the selected features\n",
        "multimodal_selected_features = X_multimodal[:, selected_multimodal_feature_indices]\n",
        "\n",
        "# 9. Print the shape of the original multimodal features and the shape of the newly selected multimodal features\n",
        "print(f\"Original number of multimodal features: {X_multimodal.shape[1]}\")\n",
        "print(f\"Number of multimodal features selected by ReliefF: {multimodal_selected_features.shape[1]}\")\n",
        "print(f\"Shape of selected multimodal features array: {multimodal_selected_features.shape}\")\n",
        "\n",
        "print(\"Multimodal ReliefF feature selection completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8199d3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error 'TypeError: ufunc 'isnan' not supported for the input types' occurred because the `X_multimodal` array contained non-numeric 'object' type columns, specifically from `aligned_tabular_features`. This happened because several categorical columns in the tabular data (`df_preprocessed_tabular_with_id`) were not fully processed into numerical format during the re-preprocessing step. To fix this, I will expand the list of columns to be LabelEncoded and explicitly convert all columns in `df_preprocessed_tabular_with_id` to numeric types, coercing any remaining non-numeric values to `NaN` and then imputing those `NaN` values, ensuring `multimodal_features` is entirely numeric before being passed to ReliefF.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e29787f"
      },
      "source": [
        "print(\"Reloading and preprocessing tabular data for alignment...\")\n",
        "\n",
        "# 1. Reload the original tabular dataset to recover 'Patient_ID'\n",
        "df_original_tabular = pd.read_csv('/content/drive/MyDrive/PhD_dataset/prostate_cancer_prediction.csv')\n",
        "\n",
        "# Make a copy to preserve original Patient_ID for merging\n",
        "df_preprocessed_tabular_with_id = df_original_tabular.copy()\n",
        "\n",
        "# --- Apply the same preprocessing steps as in cell 77c47bd7, but enhanced ---\n",
        "\n",
        "# Handle Missing Values\n",
        "print(\"Handling missing values in reloaded tabular data...\")\n",
        "numerical_cols_for_imputation = ['Age', 'PSA_Level', 'Screening_Age', 'Prostate_Volume', 'BMI'] # Added BMI\n",
        "for col in numerical_cols_for_imputation:\n",
        "    if df_preprocessed_tabular_with_id[col].isnull().any():\n",
        "        df_preprocessed_tabular_with_id[col].fillna(df_preprocessed_tabular_with_id[col].mean(), inplace=True)\n",
        "\n",
        "# Categorical columns to impute with mode\n",
        "categorical_cols = df_preprocessed_tabular_with_id.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_for_imputation = [col for col in categorical_cols if col not in ['Patient_ID']]\n",
        "\n",
        "for col in categorical_cols_for_imputation:\n",
        "    if df_preprocessed_tabular_with_id[col].isnull().any():\n",
        "        df_preprocessed_tabular_with_id[col].fillna(df_preprocessed_tabular_with_id[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"Missing values handled in reloaded tabular data.\")\n",
        "\n",
        "# Encode Categorical Features (preserving Patient_ID for now)\n",
        "print(\"Encoding categorical features in reloaded tabular data...\")\n",
        "\n",
        "# Updated list of binary categorical columns for Label Encoding\n",
        "binary_cols_reprocessed = [\n",
        "    'Family_History', 'Race_African_Ancestry', 'DRE_Result', 'Biopsy_Result',\n",
        "    'Difficulty_Urinating', 'Weak_Urine_Flow', 'Blood_in_Urine', 'Prostate_Pain',\n",
        "    'Urinary_Frequency', 'Nocturia', 'Erectile_Dysfunction', 'Weight_Loss',\n",
        "    'Bone_Pain', 'Fatigue', 'Back_Pain', 'Smoking', 'Hypertension', 'Diabetes',\n",
        "    'Follow_Up_Required', 'Genetic_Risk_Factors', 'Previous_Cancer_History', 'Early_Detection',\n",
        "    'Pelvic_Pain', 'Treatment_Recommended', 'Survival_5_Years', 'Exercise_Regularly', 'Healthy_Diet'\n",
        "]\n",
        "\n",
        "for col in binary_cols_reprocessed:\n",
        "    if col in df_preprocessed_tabular_with_id.columns:\n",
        "        unique_vals = df_preprocessed_tabular_with_id[col].unique()\n",
        "        if 'No' in unique_vals and 'Yes' in unique_vals:\n",
        "            mapping = {'No': 0, 'Yes': 1}\n",
        "            df_preprocessed_tabular_with_id[col] = df_preprocessed_tabular_with_id[col].map(mapping)\n",
        "        elif 'Normal' in unique_vals and 'Abnormal' in unique_vals:\n",
        "            mapping = {'Normal': 0, 'Abnormal': 1}\n",
        "            df_preprocessed_tabular_with_id[col] = df_preprocessed_tabular_with_id[col].map(mapping)\n",
        "        elif 'Benign' in unique_vals and 'Malignant' in unique_vals:\n",
        "            mapping = {'Benign': 0, 'Malignant': 1}\n",
        "            df_preprocessed_tabular_with_id[col] = df_preprocessed_tabular_with_id[col].map(mapping)\n",
        "        else:\n",
        "            # Fallback for other binary categories if any, though ideally handled by mappings\n",
        "            le = LabelEncoder()\n",
        "            df_preprocessed_tabular_with_id[col] = le.fit_transform(df_preprocessed_tabular_with_id[col])\n",
        "\n",
        "# Ordinal categorical columns mapping\n",
        "alcohol_mapping = {'Low': 0, 'Moderate': 1, 'High': 2}\n",
        "df_preprocessed_tabular_with_id['Alcohol_Consumption'] = df_preprocessed_tabular_with_id['Alcohol_Consumption'].map(alcohol_mapping)\n",
        "\n",
        "cholesterol_mapping = {'Low': 0, 'Normal': 1, 'High': 2}\n",
        "df_preprocessed_tabular_with_id['Cholesterol_Level'] = df_preprocessed_tabular_with_id['Cholesterol_Level'].map(cholesterol_mapping)\n",
        "\n",
        "# Handle other multi-class categorical columns using LabelEncoder\n",
        "other_categorical_cols = ['Cancer_Stage', 'Smoking_History']\n",
        "for col in other_categorical_cols:\n",
        "    if col in df_preprocessed_tabular_with_id.columns:\n",
        "        le = LabelEncoder()\n",
        "        df_preprocessed_tabular_with_id[col] = le.fit_transform(df_preprocessed_tabular_with_id[col])\n",
        "\n",
        "print(\"Categorical features encoded in reloaded tabular data.\")\n",
        "\n",
        "# Ensure all columns (except Patient_ID) are numeric, coercing errors\n",
        "print(\"Converting all feature columns to numeric...\")\n",
        "# Drop Patient_ID temporarily if it's not needed for numeric conversion check\n",
        "patiend_id_col = df_preprocessed_tabular_with_id['Patient_ID']\n",
        "df_preprocessed_tabular_with_id = df_preprocessed_tabular_with_id.drop(columns=['Patient_ID'])\n",
        "df_preprocessed_tabular_with_id = df_preprocessed_tabular_with_id.apply(pd.to_numeric, errors='coerce')\n",
        "# Fill any NaNs that might have been introduced by coercion (e.g., if a column had unexpected text values)\n",
        "df_preprocessed_tabular_with_id.fillna(df_preprocessed_tabular_with_id.mean(), inplace=True)\n",
        "# Re-add Patient_ID\n",
        "df_preprocessed_tabular_with_id['Patient_ID'] = patiend_id_col\n",
        "\n",
        "\n",
        "# Scale Numerical Features\n",
        "print(\"Scaling numerical features in reloaded tabular data...\")\n",
        "numerical_cols_for_scaling = ['Age', 'PSA_Level', 'Screening_Age', 'Prostate_Volume', 'BMI'] # Added BMI\n",
        "numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col in df_preprocessed_tabular_with_id.columns and np.issubdtype(df_preprocessed_tabular_with_id[col].dtype, np.number)]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_preprocessed_tabular_with_id[numerical_cols_for_scaling] = scaler.fit_transform(df_preprocessed_tabular_with_id[numerical_cols_for_scaling])\n",
        "\n",
        "print(\"Numerical features scaled in reloaded tabular data.\")\n",
        "print(\"Tabular data re-preprocessed with 'Patient_ID' retained.\")\n",
        "\n",
        "# 3. Merge df_metadata_valid_paths with the preprocessed tabular data\n",
        "#    Ensure Patient column in df_metadata_valid_paths is ready for merge\n",
        "\n",
        "# Create a DataFrame for image features with Patient ID\n",
        "df_image_features = pd.DataFrame(selected_features)\n",
        "df_image_features['Patient'] = df_metadata_valid_paths['Patient'].values\n",
        "df_image_features['File_Path'] = df_metadata_valid_paths['File_Path'].values\n",
        "\n",
        "print(\"Merging image features with preprocessed tabular data...\")\n",
        "# Perform the merge operation\n",
        "# We merge df_image_features (which has one row per image) with df_preprocessed_tabular_with_id (which has one row per patient)\n",
        "# The merge will duplicate tabular data for each image belonging to the same patient.\n",
        "merged_multimodal_df = pd.merge(\n",
        "    df_image_features,\n",
        "    df_preprocessed_tabular_with_id, # This now contains Patient_ID\n",
        "    left_on='Patient',\n",
        "    right_on='Patient_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Drop redundant Patient_ID column from the merged DataFrame\n",
        "merged_multimodal_df.drop('Patient_ID', axis=1, inplace=True)\n",
        "\n",
        "# Verify the merge\n",
        "print(f\"Shape of merged_multimodal_df: {merged_multimodal_df.shape}\")\n",
        "print(\"First 5 rows of merged_multimodal_df:\")\n",
        "print(merged_multimodal_df.head())\n",
        "\n",
        "# 4. Extract the aligned tabular features\n",
        "# Identify columns that are not image features or Patient/File_Path related\n",
        "# Ensure these are numeric columns\n",
        "tabular_feature_cols = [col for col in merged_multimodal_df.columns if col not in (list(range(selected_features.shape[1])) + ['Patient', 'File_Path'])]\n",
        "aligned_tabular_features = merged_multimodal_df[tabular_feature_cols].values\n",
        "\n",
        "# Ensure aligned_tabular_features is explicitly float type\n",
        "aligned_tabular_features = aligned_tabular_features.astype(float)\n",
        "\n",
        "print(f\"Shape of aligned_tabular_features: {aligned_tabular_features.shape}\")\n",
        "print(f\"Dtype of aligned_tabular_features: {aligned_tabular_features.dtype}\")\n",
        "\n",
        "# 5. Concatenate selected_features (image features) with aligned_tabular_features\n",
        "# Ensure both are numpy arrays before concatenating\n",
        "multimodal_features = np.hstack((selected_features, aligned_tabular_features))\n",
        "\n",
        "# 6. Verify the shape of the newly created multimodal feature set\n",
        "print(f\"Shape of final multimodal_features: {multimodal_features.shape}\")\n",
        "print(f\"Dtype of final multimodal_features: {multimodal_features.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38552be2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `TypeError` with `ReliefF` indicates that the `X_multimodal` array still contained non-numeric values (or values that `np.isnan` couldn't process, such as `inf`) despite the previous attempt to clean and convert. To definitively resolve this, I will add explicit checks for `NaN` and infinite values in `multimodal_features` and handle them by imputing with the mean of the respective columns, immediately before fitting the `ReliefF` model. This ensures the input to ReliefF is purely numeric and free of problematic values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c5be599"
      },
      "source": [
        "print(\"Applying ReliefF feature selection to multimodal features...\")\n",
        "\n",
        "# 1. Import ReliefF (already imported in previous cell, but re-including for clarity)\n",
        "from skrebate import ReliefF\n",
        "\n",
        "# 2. Define X_multimodal as the multimodal_features array and y_multimodal as the target labels (y)\n",
        "X_multimodal = multimodal_features\n",
        "y_multimodal = df_metadata_valid_paths['Label'].values # Ensure y_multimodal corresponds to the aligned data\n",
        "\n",
        "print(f\"Shape of original multimodal features (X_multimodal): {X_multimodal.shape}\")\n",
        "print(f\"Shape of multimodal labels (y_multimodal): {y_multimodal.shape}\")\n",
        "\n",
        "# *** NEW: Explicitly handle NaN and inf values in X_multimodal before ReliefF ***\n",
        "# Check for NaN values\n",
        "if np.isnan(X_multimodal).any():\n",
        "    print(\"NaN values found in X_multimodal. Imputing with column mean...\")\n",
        "    col_means = np.nanmean(X_multimodal, axis=0)\n",
        "    # Replace NaNs with column means, using a loop for robust replacement\n",
        "    for i in range(X_multimodal.shape[1]):\n",
        "        nan_indices = np.isnan(X_multimodal[:, i])\n",
        "        X_multimodal[nan_indices, i] = col_means[i]\n",
        "\n",
        "# Check for infinite values\n",
        "if np.isinf(X_multimodal).any():\n",
        "    print(\"Infinite values found in X_multimodal. Replacing with maximum finite value...\")\n",
        "    # Replace inf with the maximum finite value of the column, or 0 if all are inf\n",
        "    X_multimodal[np.isinf(X_multimodal)] = np.nan # Temporarily turn infs into NaNs\n",
        "    col_max_finite = np.nanmax(X_multimodal, axis=0)\n",
        "    for i in range(X_multimodal.shape[1]):\n",
        "        inf_indices = np.isinf(multimodal_features[:, i]) # Need to re-check inf if converted to NaN previously\n",
        "        # If all values in a column were inf, col_max_finite might be NaN, handle this\n",
        "        if np.isnan(col_max_finite[i]):\n",
        "            X_multimodal[inf_indices, i] = 0 # Default to 0 or another suitable value\n",
        "        else:\n",
        "            X_multimodal[inf_indices, i] = col_max_finite[i] # Replace inf with the largest finite value\n",
        "    # Now, impute NaNs that might have resulted from replacing infs with nan\n",
        "    if np.isnan(X_multimodal).any():\n",
        "        col_means_after_inf_handling = np.nanmean(X_multimodal, axis=0)\n",
        "        for i in range(X_multimodal.shape[1]):\n",
        "            nan_indices = np.isnan(X_multimodal[:, i])\n",
        "            X_multimodal[nan_indices, i] = col_means_after_inf_handling[i]\n",
        "\n",
        "\n",
        "# 3. Instantiate the ReliefF model with a suitable n_neighbors value\n",
        "# n_neighbors=10 is a common starting point\n",
        "multimodal_relief = ReliefF(n_neighbors=10)\n",
        "\n",
        "# 4. Fit the ReliefF model to X_multimodal and y_multimodal\n",
        "print(\"Fitting ReliefF model to multimodal features...\")\n",
        "multimodal_relief.fit(X_multimodal, y_multimodal)\n",
        "\n",
        "# 5. Define the desired number of features to keep (e.g., 100 or 200)\n",
        "desired_n_multimodal_features = 100 # Can adjust this value\n",
        "\n",
        "# 6. Get the sorted feature indices from the fitted ReliefF model\n",
        "sorted_multimodal_feature_indices = multimodal_relief.top_features_\n",
        "\n",
        "# 7. Select the top N feature indices\n",
        "selected_multimodal_feature_indices = sorted_multimodal_feature_indices[:desired_n_multimodal_features]\n",
        "\n",
        "# 8. Create a new feature set with only the selected features\n",
        "multimodal_selected_features = X_multimodal[:, selected_multimodal_feature_indices]\n",
        "\n",
        "# 9. Print the shape of the original multimodal features and the shape of the newly selected multimodal features\n",
        "print(f\"Original number of multimodal features: {X_multimodal.shape[1]}\")\n",
        "print(f\"Number of multimodal features selected by ReliefF: {multimodal_selected_features.shape[1]}\")\n",
        "print(f\"Shape of selected multimodal features array: {multimodal_selected_features.shape}\")\n",
        "\n",
        "print(\"Multimodal ReliefF feature selection completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6468a3bc"
      },
      "source": [
        "## Train Multimodal Classifier\n",
        "\n",
        "### Subtask:\n",
        "Split the integrated and selected multimodal features into training and testing sets, then train a suitable machine learning classifier on this multimodal dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4d32c69"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires splitting the multimodal features into training and testing sets, scaling them, and then training a RandomForestClassifier. This code block will perform all these steps sequentially.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63667a2b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"Splitting multimodal data into training and testing sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(multimodal_selected_features, y_multimodal, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"Scaling multimodal features using StandardScaler...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "\n",
        "print(\"Instantiating and training RandomForestClassifier...\")\n",
        "classifier = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"RandomForestClassifier trained successfully on scaled multimodal data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a0f1ebe"
      },
      "source": [
        "## Evaluate Classifier Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained `RandomForestClassifier` on the test set (`X_test_scaled`, `y_test`). Calculate and display standard classification metrics including accuracy, precision, recall, F1-score, and the confusion matrix. Discuss the implications of these metrics for a medical image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0598436"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires evaluating the trained `RandomForestClassifier`. This involves making predictions on the scaled test set, calculating the classification report (accuracy, precision, recall, F1-score), and generating a confusion matrix. These steps will be performed sequentially within a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7e601ba"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Evaluating classifier performance...\")\n",
        "\n",
        "# Make predictions on the scaled test set\n",
        "y_pred = classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate and display confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.title('Confusion Matrix for Multimodal Classifier')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"Classifier performance evaluation complete.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de23459a"
      },
      "source": [
        "### Discussion of Classifier Performance Metrics\n",
        "\n",
        "The evaluation metrics provide insight into the performance of our multimodal classifier for medical image classification.\n",
        "\n",
        "**Classification Report Analysis:**\n",
        "*   **Accuracy (0.9222):** The overall accuracy is 92.22%, which indicates that the model correctly classifies a high percentage of both positive and negative cases. While a high accuracy is generally good, in medical contexts, it's crucial to look beyond overall accuracy, especially if there's an imbalance in class distribution (which is often the case with disease detection).\n",
        "*   **Precision (Class 0: 0.91, Class 1: 0.95):**\n",
        "    *   For class 0 (e.g., 'No Disease'), a precision of 0.91 means that when the model predicts 'No Disease', it is correct 91% of the time. This is good, minimizing false positives for healthy individuals.\n",
        "    *   For class 1 (e.g., 'Disease'), a precision of 0.95 means that when the model predicts 'Disease', it is correct 95% of the time. This is a very high precision for detecting the positive class, indicating a low rate of false positives for disease (i.e., fewer healthy patients are misdiagnosed with the disease).\n",
        "*   **Recall (Class 0: 0.99, Class 1: 0.75):**\n",
        "    *   For class 0, a recall of 0.99 means the model correctly identifies 99% of all actual 'No Disease' cases. This is excellent for ensuring that truly healthy individuals are not missed.\n",
        "    *   For class 1, a recall of 0.75 means the model correctly identifies 75% of all actual 'Disease' cases. This implies that 25% of actual disease cases are being missed (false negatives). In medical diagnosis, especially for serious conditions like cancer, a high recall for the positive class is often paramount to avoid missing actual cases.\n",
        "*   **F1-score (Class 0: 0.95, Class 1: 0.84):** The F1-score is the harmonic mean of precision and recall. A high F1-score indicates a good balance between precision and recall. While the F1-score for class 0 is very high (0.95), for class 1, it is 0.84, which is decent but indicates room for improvement, particularly due to the lower recall for this class.\n",
        "\n",
        "**Confusion Matrix Analysis:**\n",
        "```\n",
        "[[838  12]\n",
        " [ 79 240]]\n",
        "```\n",
        "*   **True Negatives (TN): 838** The model correctly identified 838 cases as 'No Disease'.\n",
        "*   **False Positives (FP): 12** The model incorrectly identified 12 cases as 'Disease' when they were actually 'No Disease'. (Type I error)\n",
        "*   **False Negatives (FN): 79** The model incorrectly identified 79 cases as 'No Disease' when they were actually 'Disease'. (Type II error)\n",
        "*   **True Positives (TP): 240** The model correctly identified 240 cases as 'Disease'.\n",
        "\n",
        "**Implications for Medical Image Classification:**\n",
        "\n",
        "In medical image classification, the balance between minimizing false positives and false negatives is critical and often depends on the specific context and the severity of the condition being diagnosed.\n",
        "\n",
        "*   **High False Negatives (79 cases):** The most significant concern here is the relatively high number of false negatives (79). Missing a disease (Type II error) can have severe consequences for patients, leading to delayed treatment and potentially worse outcomes. For a medical diagnostic tool, especially one dealing with conditions like cancer, a recall of 75% for the positive class might be considered too low. It means that 25% of patients who *do* have the condition are being told they don't.\n",
        "*   **Low False Positives (12 cases):** The model has a low number of false positives (12), which is good. A high number of false positives can lead to unnecessary anxiety, further costly and invasive diagnostic procedures, and potentially harmful treatments for healthy individuals.\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "The classifier demonstrates strong performance in identifying negative cases (high recall for class 0) and high precision when predicting positive cases. However, the lower recall for the positive class (i.e., the disease) is a critical area for improvement. In many medical scenarios, prioritizing high recall for the disease class (even at the cost of slightly higher false positives) is preferred to ensure that no patient with the condition is missed. Future work should focus on strategies to reduce false negatives, possibly through:\n",
        "\n",
        "*   **Data Augmentation:** Increasing the diversity of the minority class data.\n",
        "*   **Cost-Sensitive Learning:** Assigning higher penalties for false negatives during training.\n",
        "*   **Threshold Adjustment:** Modifying the classification threshold to favor higher recall.\n",
        "*   **Ensemble Methods/Other Models:** Experimenting with other advanced models or ensemble techniques that might offer a better balance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fa5fff9"
      },
      "source": [
        "### Discussion of Classifier Performance Metrics\n",
        "\n",
        "The evaluation metrics provide insight into the performance of our multimodal classifier for medical image classification.\n",
        "\n",
        "**Classification Report Analysis:**\n",
        "*   **Accuracy (0.9222):** The overall accuracy is 92.22%, which indicates that the model correctly classifies a high percentage of both positive and negative cases. While a high accuracy is generally good, in medical contexts, it's crucial to look beyond overall accuracy, especially if there's an imbalance in class distribution (which is often the case with disease detection).\n",
        "*   **Precision (Class 0: 0.91, Class 1: 0.95):**\n",
        "    *   For class 0 (e.g., 'No Disease'), a precision of 0.91 means that when the model predicts 'No Disease', it is correct 91% of the time. This is good, minimizing false positives for healthy individuals.\n",
        "    *   For class 1 (e.g., 'Disease'), a precision of 0.95 means that when the model predicts 'Disease', it is correct 95% of the time. This is a very high precision for detecting the positive class, indicating a low rate of false positives for disease (i.e., fewer healthy patients are misdiagnosed with the disease).\n",
        "*   **Recall (Class 0: 0.99, Class 1: 0.75):**\n",
        "    *   For class 0, a recall of 0.99 means the model correctly identifies 99% of all actual 'No Disease' cases. This is excellent for ensuring that truly healthy individuals are not missed.\n",
        "    *   For class 1, a recall of 0.75 means the model correctly identifies 75% of all actual 'Disease' cases. This implies that 25% of actual disease cases are being missed (false negatives). In medical diagnosis, especially for serious conditions like cancer, a high recall for the positive class is often paramount to avoid missing actual cases.\n",
        "*   **F1-score (Class 0: 0.95, Class 1: 0.84):** The F1-score is the harmonic mean of precision and recall. A high F1-score indicates a good balance between precision and recall. While the F1-score for class 0 is very high (0.95), for class 1, it is 0.84, which is decent but indicates room for improvement, particularly due to the lower recall for this class.\n",
        "\n",
        "**Confusion Matrix Analysis:**\n",
        "```\n",
        "[[838  12]\n",
        " [ 79 240]]\n",
        "```\n",
        "*   **True Negatives (TN): 838** The model correctly identified 838 cases as 'No Disease'.\n",
        "*   **False Positives (FP): 12** The model incorrectly identified 12 cases as 'Disease' when they were actually 'No Disease'. (Type I error)\n",
        "*   **False Negatives (FN): 79** The model incorrectly identified 79 cases as 'No Disease' when they were actually 'Disease'. (Type II error)\n",
        "*   **True Positives (TP): 240** The model correctly identified 240 cases as 'Disease'.\n",
        "\n",
        "**Implications for Medical Image Classification:**\n",
        "\n",
        "In medical image classification, the balance between minimizing false positives and false negatives is critical and often depends on the specific context and the severity of the condition being diagnosed.\n",
        "\n",
        "*   **High False Negatives (79 cases):** The most significant concern here is the relatively high number of false negatives (79). Missing a disease (Type II error) can have severe consequences for patients, leading to delayed treatment and potentially worse outcomes. For a medical diagnostic tool, especially one dealing with conditions like cancer, a recall of 75% for the positive class might be considered too low. It means that 25% of patients who *do* have the condition are being told they don't.\n",
        "*   **Low False Positives (12 cases):** The model has a low number of false positives (12), which is good. A high number of false positives can lead to unnecessary anxiety, further costly and invasive diagnostic procedures, and potentially harmful treatments for healthy individuals.\n",
        "\n",
        "**Overall Assessment:**\n",
        "\n",
        "The classifier demonstrates strong performance in identifying negative cases (high recall for class 0) and high precision when predicting positive cases. However, the lower recall for the positive class (i.e., the disease) is a critical area for improvement. In many medical scenarios, prioritizing high recall for the disease class (even at the cost of slightly higher false positives) is preferred to ensure that no patient with the condition is missed. Future work should focus on strategies to reduce false negatives, possibly through:\n",
        "\n",
        "*   **Data Augmentation:** Increasing the diversity of the minority class data.\n",
        "*   **Cost-Sensitive Learning:** Assigning higher penalties for false negatives during training.\n",
        "*   **Threshold Adjustment:** Modifying the classification threshold to favor higher recall.\n",
        "*   **Ensemble Methods/Other Models:** Experimenting with other advanced models or ensemble techniques that might offer a better balance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af91c7f8"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire multimodal classification pipeline, including the integration of tabular and image features, the feature selection process, the training of the classifier, and the evaluation results. Provide insights into the model's performance and any advantages gained from the multimodal approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dd025dd"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "**1. Summarize the entire multimodal classification pipeline, including the integration of tabular and image features, the feature selection process, the training of the classifier, and the evaluation results.**\n",
        "The pipeline involved several key stages:\n",
        "*   **Tabular Data Preprocessing:** The `prostate_cancer_prediction.csv` data was loaded. Missing numerical values were imputed with the mean, and missing categorical values with the mode. Categorical features were encoded (binary via direct mapping, ordinal via direct mapping, and other multi-class via `LabelEncoder`). Numerical features were scaled using `StandardScaler`.\n",
        "*   **Multimodal Feature Integration:** To combine tabular and image features, the tabular dataset was reloaded to retain the `Patient_ID` column, and the preprocessing steps were reapplied. Image features (from `selected_features`) were associated with their respective patients. These were then merged with the preprocessed tabular data, matching on `Patient_ID`, to create a single multimodal feature set.\n",
        "*   **Multimodal ReliefF Feature Selection:** The combined multimodal features underwent a crucial data cleaning step to handle `NaN` and infinite values (imputing NaNs with column mean and replacing infinities with maximum finite values) before applying the ReliefF algorithm. ReliefF selected the top 100 most discriminative features, reducing the dimensionality from 229 to 100 features.\n",
        "*   **Classifier Training & Evaluation:** The selected multimodal features were split into 80% training and 20% testing sets. Features were scaled using `StandardScaler`, and a `RandomForestClassifier` was trained. Evaluation was performed using accuracy, precision, recall, F1-score, and a confusion matrix.\n",
        "\n",
        "**2. Provide insights into the model's performance and any advantages gained from the multimodal approach.**\n",
        "The multimodal classifier achieved an overall accuracy of 92.22%. It demonstrated high precision for both classes (0.91 for class 0/negative, 0.95 for class 1/positive) and excellent recall for the negative class (0.99). However, the recall for the positive class (disease detection) was 0.75, meaning 25% of actual disease cases were missed (79 false negatives). While the multimodal approach wasn't directly compared to unimodal models in this summary, the integration of diverse data types (tabular and image features) typically provides a more comprehensive view of the patient's condition, which can lead to a more robust and potentially more accurate diagnostic tool by leveraging complementary information that might not be apparent from a single data source. The ability to select the most relevant features from this combined dataset using ReliefF further refines the model's focus on discriminative characteristics across modalities.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Tabular Data Preprocessing:** Missing values in numerical columns (`Age`, `PSA_Level`, `Screening_Age`, `Prostate_Volume`) were imputed with the mean, and categorical columns with the mode. Binary and ordinal categorical features were encoded, and numerical features were scaled. The `Patient_ID` column was initially dropped but later preserved for data integration.\n",
        "*   **Multimodal Feature Integration:** A crucial step involved reloading the tabular data to retain `Patient_ID` for accurate merging. The `selected_features` (image features) were combined with the re-preprocessed tabular features based on `Patient_ID`, resulting in a `multimodal_features` array of shape (5842, 229). This array contained 200 image features and 29 tabular features.\n",
        "*   **Multimodal ReliefF Feature Selection:** Explicit handling of `NaN` and infinite values in the `multimodal_features` array was necessary before applying ReliefF. The ReliefF algorithm successfully reduced the feature dimensionality from 229 to 100, yielding `multimodal_selected_features` with a shape of (5842, 100).\n",
        "*   **Classifier Performance:**\n",
        "    *   The `RandomForestClassifier` achieved an accuracy of 0.9222.\n",
        "    *   For the negative class (class 0), precision was 0.91, recall was 0.99, and F1-score was 0.95.\n",
        "    *   For the positive class (class 1), precision was 0.95, recall was 0.75, and F1-score was 0.84.\n",
        "    *   The confusion matrix showed 838 True Negatives, 12 False Positives, 79 False Negatives, and 240 True Positives.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Prioritize Recall for Disease Detection:** While the model exhibits high overall accuracy and precision, the relatively low recall (0.75) for the positive class (disease) is a significant concern in medical diagnostics. Future efforts should focus on strategies to reduce false negatives, potentially through cost-sensitive learning, adjusting the classification threshold, or exploring advanced ensemble methods.\n",
        "*   **Investigate Feature Contributions:** Analyzing the importance scores of the 100 selected multimodal features from ReliefF could provide insights into which specific image features or tabular features are most discriminative for the diagnosis. This could help in understanding the biological or clinical relevance of the chosen features and potentially guide further feature engineering or data collection efforts.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrAkzMO3sGYK6ECXy4rctJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}